{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Kernel Methods and SVMs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e3dc5da93107530"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First install and import needed packages and libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8dfc27660a70a10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install numpy scipy pandas matplotlib scikit-learn missingno"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e28329d1301d4702"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "778c76934493fd6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We're going to use the [Iranian Churn Dataset](https://archive.ics.uci.edu/ml/datasets/Iranian+Churn+Dataset) from [Kaggle](https://www.kaggle.com/datasets/royjafari/customer-churn). \n",
    "\n",
    "The dataset was created with real world data of an Iranian telecom company. It provides an information about whether the company's client left them or not.\n",
    "\n",
    "Such dataset can be used to help improving client's satisfaction and maximizing company's earnings.\n",
    "\n",
    "The dataset contains information about costs of False Positive and False Negative classification for each client."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed422121f2a448a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "churn_df = pd.read_csv(\"customer_churn_data.csv\")\n",
    "churn_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcb22a463169d876"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = churn_df.pop(\"Churn\")\n",
    "fn_cost = churn_df.pop(\"FN\")\n",
    "fp_cost = churn_df.pop(\"FP\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca3b5e98429eff79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = y.value_counts()[[0, 1]].plot.bar(\n",
    "    x=\"class\", y=\"Number of clients\", rot=0, title=\"Class distribution\"\n",
    ")\n",
    "ax.set(xlabel=\"Class\", ylabel=\"Number of clients\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "707ea5a98e4deddd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's split and scale our data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1dd9b877617e260"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    fp_cost_train,\n",
    "    fp_cost_test,\n",
    "    fn_cost_train,\n",
    "    fn_cost_test,\n",
    ") = train_test_split(\n",
    "    churn_df, y, fp_cost, fn_cost, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd7589e354dcd5e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a linear regression to use as our baseline for further comparison"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0cbf843d2cf8da3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logreg_cv_l2 = LogisticRegressionCV(\n",
    "    Cs=100, cv=5, scoring=\"f1\", class_weight=\"balanced\", random_state=0, n_jobs=-1\n",
    ")\n",
    "logreg_cv_l2.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg_cv_l2.predict(X_test)\n",
    "print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c76eb44f2de5e9f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train a linear SVM classifier and see how it compares with linear regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db6353e139e1e95e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(\n",
    "    loss=\"hinge\", max_iter=10000, random_state=0, class_weight=\"balanced\", dual=\"auto\"\n",
    ")\n",
    "\n",
    "param_grid = {\"C\": np.power(10.0, np.arange(-2, 2, 4 / 100))}\n",
    "\n",
    "cv = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
    "\n",
    "start_time = time()\n",
    "cv.fit(X_train, y_train)\n",
    "end_time = time()\n",
    "\n",
    "prediction_time = end_time - start_time\n",
    "\n",
    "print(\"training time: \", prediction_time, \"s\")\n",
    "print(\"best C: \", cv.best_params_[\"C\"])\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffe120caf3035126"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both baseline score and SVM score are not great. Let's see if we can improve the score with Kernel SVM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbc41dbac3960d31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_kernel_svc = SVC(cache_size=512, class_weight=\"balanced\", random_state=0)\n",
    "start_time = time()\n",
    "clf_kernel_svc.fit(X_train, y_train)\n",
    "end_time = time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "y_pred = clf_kernel_svc.predict(X_test)\n",
    "\n",
    "print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")\n",
    "print(f\"Training time: {training_time} s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6986c66101bf3079"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The improvement is noticeable but still not as great as we would like.\n",
    "\n",
    "Maybe tuning the hyperparameters will help? We'll try a few different kernels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2da64552703893f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = SVC(cache_size=512, class_weight=\"balanced\", random_state=0)\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"kernel\": [\"rbf\"],\n",
    "        \"C\": np.linspace(1e-2, 1e2, num=20),\n",
    "        \"gamma\": [\"scale\"] + list(np.linspace(1e-1, 1e1, num=20)),\n",
    "    },\n",
    "    {\n",
    "        \"kernel\": [\"poly\"],\n",
    "        \"C\": np.linspace(1e-3, 1e3, 100),\n",
    "        \"degree\": [2, 3, 4, 5],\n",
    "    },\n",
    "    {\n",
    "        \"kernel\": [\"sigmoid\"],\n",
    "        \"C\": np.linspace(1e-2, 1e2, num=20),\n",
    "        \"gamma\": [\"scale\"] + list(np.linspace(1e-1, 1e1, num=20)),\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "cv = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
    "\n",
    "start_time = time()\n",
    "cv.fit(X_train, y_train)\n",
    "end_time = time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")\n",
    "print(f\"Training time: {training_time} s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a1d15c6f4f9e43c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The improvement is tremendous. Let's see what parameters enabled us to get this score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58bf874021f9fe2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for param in cv.best_params_:\n",
    "    print(f\"best {param} : {cv.best_params_[param]}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95613f1a00954c68"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's see the best validation score for each kernel!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64627c9baac2ddd1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(cv.cv_results_)\n",
    "\n",
    "kernels = [\"rbf\", \"poly\", \"sigmoid\"]\n",
    "\n",
    "for k in kernels:\n",
    "    score = results_df.loc[results_df[\"param_kernel\"] == k][\"mean_test_score\"].max()\n",
    "    print(f\"Best validation F1-score for {k} : {100 * score:.2f}%\")\n",
    "    print(results_df.loc[results_df[\"mean_test_score\"] == score].get(\"params\").iloc[0])\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1767924c90c06cda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay. Let's try to train a model with these parameters and see the test score for each model "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81b9266100c05435"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for k in kernels:\n",
    "    score = results_df.loc[results_df[\"param_kernel\"] == k][\"mean_test_score\"].max()\n",
    "    params = (\n",
    "        results_df.loc[results_df[\"mean_test_score\"] == score].get(\"params\").iloc[0]\n",
    "    )\n",
    "\n",
    "    clf_kernel_svc = SVC(\n",
    "        cache_size=512,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=0,\n",
    "        **params,\n",
    "    )\n",
    "    start_time = time()\n",
    "    clf_kernel_svc.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    y_pred = clf_kernel_svc.predict(X_test)\n",
    "    print(f\"F1-score for {k} : {100 * f1_score(y_test, y_pred):.2f}%\")\n",
    "    print(f\"Training time: {training_time} s\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad846b878bccf489"
  },
  {
   "cell_type": "markdown",
   "source": [
    "During the validation, the rbf kernel performed the best. However, the testing shows that the polynomial kernel worked better\n",
    "\n",
    "Let's save the best parameters for later"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c05d08db33468472"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score = results_df.loc[results_df[\"param_kernel\"] == \"poly\"][\"mean_test_score\"].max()\n",
    "best_kernel_params = (\n",
    "    results_df.loc[results_df[\"mean_test_score\"] == score].get(\"params\").iloc[0]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ea407adf955f27f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Now let's try to create a scoring function that will take into account the FN and FP values "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe6dce5f8bcdc9c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "def churn_cost(\n",
    "    y_true: Union[np.ndarray, pd.Series],\n",
    "    y_pred: Union[np.ndarray, pd.Series],\n",
    "    fp_cost: Union[np.ndarray, pd.Series],\n",
    "    fn_cost: Union[np.ndarray, pd.Series],\n",
    ") -> float:\n",
    "    # make sure all data is Numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    fp_cost = np.array(fp_cost)\n",
    "    fn_cost = np.array(fn_cost)\n",
    "\n",
    "    # check which rows are for FP and FN\n",
    "    FP = (y_true == 0) & (y_pred == 1)\n",
    "    FN = (y_true == 1) & (y_pred == 0)\n",
    "\n",
    "    # get costs for FP and FN\n",
    "    fp_real_cost = FP * fp_cost\n",
    "    fn_real_cost = FN * fn_cost\n",
    "\n",
    "    # return sum of costs\n",
    "    return fp_real_cost.sum() + fn_real_cost.sum()\n",
    "\n",
    "\n",
    "churn_cost_score = make_scorer(\n",
    "    churn_cost,\n",
    "    average=\"macro\",\n",
    "    greater_is_better=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6d76765e93de051"
  },
  {
   "cell_type": "markdown",
   "source": [
    "now let's create two dummy classifiers. One that will always predict the positive class and one that always predicts negative"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41385fe685550904"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def DummyPredict(X, churn):\n",
    "    return np.full(X.shape[0], churn)\n",
    "\n",
    "\n",
    "y_pred_p = DummyPredict(X_test, 1)\n",
    "y_pred_n = DummyPredict(X_test, 0)\n",
    "\n",
    "print(\n",
    "    f\"F1-score for all positive predictions : {100 * f1_score(y_test, y_pred_p):.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"F1-score for all negative predictions : {100 * f1_score(y_test, y_pred_n):.2f}%\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Cost score for all positive predictions : {churn_cost(y_test, y_pred_p, fp_cost_test, fn_cost_test):.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Cost score for all negative predictions : {churn_cost(y_test, y_pred_n, fp_cost_test, fn_cost_test):.2f}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b9d6689e142bb93"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that unfortunately the cost of letting all unsatisfied clients go is much lower than the cost of trying to assume that all clients might leave\n",
    "\n",
    "How can SVM help us lower that cost?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99da56c248b7923b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lin_clf = LinearSVC(\n",
    "    loss=\"hinge\",\n",
    "    max_iter=10000,\n",
    "    random_state=0,\n",
    "    class_weight=\"balanced\",\n",
    "    C=52.48074602497766,\n",
    ")\n",
    "lin_clf.fit(X_train, y_train)\n",
    "y_pred = lin_clf.predict(X_test)\n",
    "print(f\"churn cost: {churn_cost(y_test, y_pred, fp_cost_test, fn_cost_test):.2f}\")\n",
    "\n",
    "\n",
    "clf_kernel_svc = SVC(\n",
    "    cache_size=512,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=0,\n",
    "    **best_kernel_params,\n",
    ")\n",
    "clf_kernel_svc.fit(X_train, y_train)\n",
    "y_pred = clf_kernel_svc.predict(X_test)\n",
    "print(f\"churn cost: {churn_cost(y_test, y_pred, fp_cost_test, fn_cost_test):.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdbd67dbf4d191e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can lower the cost over 4 times! This means very large fund savings especially for companies that operate on a very large scale"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f008f6d08f56289"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel approximation - TODO\n",
    "\n",
    "W przypadku, kiedy zastosowanie kernel SVM wprost byłoby zbyt kosztowne, można zamiast tego przybliżyć cechy kernelowe. Realizują to algorytmy grupy kernel approximation.\n",
    "\n",
    "Metoda Nyströma to algorytm uniwersalny, który potrafi przybliżyć dowolny kernel. Wykorzystuje do tego truncated SVD, czyli przybliża wprost kernel matrix za pomocą macierzy niższej rangi. Ceną tej prostoty i uniwersalności jest nie najlepsza prędkość działania w porównaniu do bardziej specjalistycznych algorytmów. Proste wyprowadzenie tej metody można znaleźć [tutaj](https://stats.stackexchange.com/questions/261149/nystroem-method-for-kernel-approximation).\n",
    "\n",
    "`RBFSampler` w Scikit-learn implementuje metodę Random Kitchen Sinks. Ten algorytm za pomocą próbkowania Monte Carlo przybliża coraz lepiej macierz kernela RBF. W bibliotece Scikit-learn-extra zaimplementowane jest rozwinięcie tego algorytmu o nazwie [Fastfood](https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.kernel_approximation.Fastfood.html), który stosuje nieco bardziej zaawansowany algorytm przybliżający, który powinien być jeszcze szybszy i oszczędniejszy pamięciowo.\n",
    "\n",
    "Hiperparametry tych metod są podobne do zwykłego kernel SVM, ale dodatkowo mamy wymiarowość naszego przybliżenia, czyli `n_components`. Im większe, tym dokładniej przybliżamy, ale też tym więcej mamy cech. Wejściem do metody Nyströma jest macierz cech X, a wyjściem macierz cech kernelowych X mająca rozmiar `n_samples * n_components`. Na takich cechach trenuje się już zwykły liniowy SVM.\n",
    "\n",
    "1. Zaimplementuj `Pipeline` składający się z przybliżenia kernela oraz liniowego SVM: dla metody Nyströma, oraz dla obu algorytmów dla kernela RBF.\n",
    "2. Wytrenuj algorytmy z domyślnymi hiperparametrami. Porównaj jakość predykcji oraz szybkość z liniowym SVM i z kernel SVM. Czy warto dokonać takiej aproksymacji?\n",
    "3. Dokonaj optymalizacji hiperparametrów `C`, `gamma` oraz `n_components`. Czy udało się poprawić wynik względem zwykłego kernel SVM w rozsądnym czasie?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d1bef582c15f82e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "edf4f6251b24b6f0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
