{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Kernel Methods and SVMs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e3dc5da93107530"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First install and import needed packages and libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8dfc27660a70a10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install numpy scipy pandas matplotlib scikit-learn missingno"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e28329d1301d4702"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "778c76934493fd6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We're going to use [Cod-RNA dataset](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#cod-rna) from ([this article](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-173)):\n",
    "\n",
    "In biology, RNA transports genetic information from DNA to proteins. However, not all RNA and DNA are coding. Other types, such as non-coding RNA, have many other functions.\n",
    "\n",
    "There are hypotheses suggesting that the secondary structure of genetic material can be used to determine whether RNA is coding. Predicting the higher degree structure of genetic material is a typical machine learning task. As it's expensive to test that experimentally.\n",
    "\n",
    "The dataset we're going to use for this task has 8 features:\n",
    "- the value calculated by a [Dynalign](https://www.sciencedirect.com/science/article/abs/pii/S0022283601953513)\n",
    "- The length of the shortest genetic sequence\n",
    "- Nucleobase frequencies in sequences 1 and 2 (6 features total)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3567c4e69b20d8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_data = pd.read_parquet(\"cod_rna_train.parquet\")\n",
    "test_data = pd.read_parquet(\"cod_rna_test.parquet\")\n",
    "y_train, y_test = train_data[\"y\"], test_data[\"y\"]\n",
    "\n",
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4aa2cd7278558f53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = y_train.value_counts()[[-1, 1]].plot.bar(\n",
    "    x=\"class\", y=\"number of elements\", rot=0, title=\"class distribution\"\n",
    ")\n",
    "ax.set(xlabel=\"Class\", ylabel=\"Number of elements\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4032d948f4e49c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We're going to perform a lightly unbalanced binary classification. As a metric we'll use the F1-score\n",
    "\n",
    "Let's scale the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34d62fa75888426"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((-1, 1))\n",
    "X_train_scaled = scaler.fit_transform(train_data.drop(\"y\", axis=1))\n",
    "X_test_scaled = scaler.transform(test_data.drop(\"y\", axis=1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff5f331d99df42d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a linear regression to use as our baseline for further comparison with SVMs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cf905bb3b00e5e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logreg_cv_l2 = LogisticRegressionCV(\n",
    "    Cs=100, cv=5, scoring=\"f1\", class_weight=\"balanced\", random_state=0, n_jobs=-1\n",
    ")\n",
    "logreg_cv_l2.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = logreg_cv_l2.predict(X_test_scaled)\n",
    "print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3bdf5588e3140ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Typical SVM\n",
    "Now that our data is prepared let's train a typical support vector machine model\n",
    "The default loss function for scikit-learn's LinearSVC is squared hinge loss. Let's change that to hinge loss to get a typical SVM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63355d2a2fabeab0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_linear_svc = LinearSVC(loss=\"hinge\", max_iter=10000, random_state=0, dual=\"auto\")\n",
    "clf_linear_svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf_linear_svc.predict(X_test_scaled)\n",
    "print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18aa1b6810001a95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that even without hyperparameter tuning we can achieve comparable results\n",
    "\n",
    "Let's see what we can get with hyperparameter tuning!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0ec4454049d924"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = LinearSVC(max_iter=10000, random_state=0, dual=\"auto\")\n",
    "\n",
    "param_grid = {\n",
    "    # \"C\": [0.001, 0.01, 0.1, 1., 10, 100]\n",
    "    # \"C\": np.arange(1,100,10)\n",
    "    \"C\": np.arange(60, 70, 1),\n",
    "    \"loss\": [\"hinge\", \"squared_hinge\"],\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
    "\n",
    "cv.fit(X_train_scaled, y_train)\n",
    "print(\"best C\", cv.best_params_[\"C\"])\n",
    "print(\"optimal loss\", cv.best_params_[\"loss\"])\n",
    "\n",
    "y_pred = cv.predict(X_test_scaled)\n",
    "print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebe478c05c40e3ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that our scores are not much better than before. However, in the article authors use kernel SVM. Let's see if that can help us improve our scores\n",
    "\n",
    "Kernel svm is implemented in SVC class. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "504017c1e3b9a71f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_kernel_svc = SVC(cache_size=512, class_weight=\"balanced\", random_state=0)\n",
    "\n",
    "start_training = time()\n",
    "clf_kernel_svc.fit(X_train_scaled, y_train)\n",
    "end_training = time()\n",
    "\n",
    "start_prediction = time()\n",
    "y_pred = clf_kernel_svc.predict(X_test_scaled)\n",
    "end_prediction = time()\n",
    "\n",
    "print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")\n",
    "print(f\"Training time   : {end_training - start_training} s\")\n",
    "print(f\"Prediction time : {end_prediction - start_prediction} s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6e26153ae2b470f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The high time results from the complexity of the kernel SVM - $O(n^2)$ \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cdfafcadc813910"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "decision_function = clf_kernel_svc.decision_function(X_train_scaled)\n",
    "end_time = time()\n",
    "prediction_time = end_time - start_time\n",
    "print(f\"Prediction time: {prediction_time} s\")\n",
    "\n",
    "support_vector_indices = np.where(np.abs(decision_function) <= 1 + 1e-15)[0]\n",
    "\n",
    "print(\"number of support vectors            : \", len(support_vector_indices))\n",
    "print(\n",
    "    \"support vectors in the training data : \",\n",
    "    len(support_vector_indices) / X_train_scaled.shape[0],\n",
    "    \"%\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "694e096a8fcb5775"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check that the prediction time is much shorter for linear regression and linear SVM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5e69901a352ccff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "_ = clf_linear_svc.decision_function(X_train_scaled)\n",
    "end_time = time()\n",
    "prediction_time = end_time - start_time\n",
    "print(f\"Prediction time: {prediction_time} s\")\n",
    "\n",
    "start_time = time()\n",
    "_ = logreg_cv_l2.decision_function(X_train_scaled)\n",
    "end_time = time()\n",
    "\n",
    "prediction_time = end_time - start_time\n",
    "print(f\"Prediction time: {prediction_time} s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32561bc89a67b5f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "639a07078f233021"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
